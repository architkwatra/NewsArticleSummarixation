# -*- coding: utf-8 -*-
"""ALDA_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L2M0J8Ie8yljpt66wN1hhcWnXKMOBJGL
"""

from google.colab import drive
drive.mount('/content/drive')
!ls
!pwd
!cd /content/drive
print('done')



import pandas as pd
import matplotlib.pyplot as plt   
import numpy as np

# df1=pd.read_csv("/content/drive/My Drive/RK_ALDA_PROJ/release/50000.csv")
# df2=pd.read_csv('/content/drive/My Drive/RK_ALDA_PROJ/release/300000.csv')
# df3=pd.read_csv('/content/drive/My Drive/RK_ALDA_PROJ/release/400000.csv')
# df4=pd.read_csv('/content/drive/My Drive/RK_ALDA_PROJ/release/700000.csv')
# df5=pd.read_csv('/content/drive/My Drive/RK_ALDA_PROJ/release/till_last.csv')
# main=pd.concat([df1,df2,df3,df4,df5],axis=0)

data=pd.read_csv("/content/drive/My Drive/RK_ALDA_PROJ/RandomSample.csv",index_col=0)



data.head()

density = data["density"]

y = []
for i in range(1, 500001):
  y.append(i)

d = density.values.tolist()

d[0:5]

plt.hist(d[0:100000],bins='auto', histtype="step")
plt.xlim(0, 40)
plt.xlabel('row number') 
plt.ylabel('summary density') 
plt.title('initial density plot') 
plt.show()

#COVERAGE PLOT
# plt.hist(d,bins=100)
# plt.xlim(0, 125)
  
# plt.xlabel('row number') 
# plt.ylabel('summary density') 
  
# plt.title('initial density plot') 
  
# plt.show() 

# from matplotlib.pyplot import figure
# figure(figsize=(80,6))

# plt.plot(y[0:100000],c[0:100000]) 
  
# plt.xlabel('row number') 
# plt.ylabel('summary coverage') 

# plt.title('initial coverage plot') 
# plt.show()

# for index, row in data[0:5].iterrows(): # actual code -> for row in data:
  
# #   for word in row["text"]:
#   print(row["text"])
#   print("\n")
# (((data[0:1]["text"][0][1:len(data[0:1]["text"][0]) - 1]).replace("'", "")).split(", "))

#Hyper Parameters for word2vec
settings = {
	'window_size': 200,	# context window +- center word , size varies from 100-300
	'n': 10,		# dimensions of word embeddings, also refer to size of hidden layer
	'epochs': 50,		# number of training epochs
	'learning_rate': 0.01	# learning rate
}

#declaring used variables
from collections import defaultdict
word_counts = {}
word_counts = defaultdict(lambda: 0, word_counts)
v_count = 0
words_list = []
word_index = {}
index_word = {}
word_vec = {}
window = settings["window_size"]

for index, row in data[0:1].iterrows():
  for word in row["text"][1:len(row["text"]) - 1].replace("'", "").split(", "): 
    print(word)
print(type(data))

def generate_training_data(settings, corpus):
#   for row in corpus.iterrows(): # actual code -> for row in data:
#     for word in row["text"]:
  for index, row in corpus.iterrows():
    for word in row["text"][1:len(row["text"]) - 1].replace("'", "").split(", "):
      if word.isalnum():
        word_counts[word] += 1
    ## How many unique words in vocab? 9
    v_count = len(word_counts.keys())
    # Generate Lookup Dictionaries (vocab)
    words_list = list(word_counts.keys())
    # Generate word:index
    word_index = dict((word, i) for i, word in enumerate(words_list))
    # Generate index:word
    index_word = dict((i, word) for i, word in enumerate(words_list))

    training_data = []
    # Cycle through each sentence in corpus
    for sentence in corpus["text"]:
      sent_len = len(sentence)
      # Cycle through each word in sentence
      for i, word in enumerate(sentence):
        # Convert target word to one-hot
        w_target = word2onehot(sentence[i])
        # Cycle through context window
        w_context = []
        # Note: window_size 2 will have range of 5 values
        for j in range(i - window, i + window+1):
          # Criteria for context word 
          # 1. Target word cannot be context word (j != i)
          # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range
          # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range 
          if j != i and j <= sent_len-1 and j >= 0:
            # Append the one-hot representation of word to w_context
            w_context.append(word2onehot(sentence[j]))
            # print(sentence[i], sentence[j]) 
            # training_data contains a one-hot representation of the target word and context words
        training_data.append([w_target, w_context])
    return np.array(training_data)

import numpy as np
def word2onehot(word):
#   print("word =", word)
  global word_index
  global word_vec
  if word.isalnum():
    
    # word_vec - initialise a blank vector
    word_vec = [0 for i in range(0, v_count)] 
    # Get ID of word from word_index
    
    if word in word_index:
      print(word_index)
      w = word_index[word]
    # Change value from 0 to 1 according to ID of the word
#     print((word_index))
      word_vec[w] = 1
  return word_vec

training_data = generate_training_data(settings, data[0:10])

training_data



# new.shape

# new

# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize

# nltk.download('stopwords')

# nltk.download('punkt')

# stop_words=set(stopwords.words('english'))

# dic={}
for e in stop_words:
  dic[e]=0

# 'an' in stop_words

# nltk.download('wordnet')

# from nltk.stem import PorterStemmer as ps
# ps = PorterStemmer()

# from nltk.stem import WordNetLemmatizer
# lemmatizer = WordNetLemmatizer()

# import re

# counter=0

# def preprocessing(x):
# #   print(x)
#   tokens=word_tokenize(x)
# #   print(tokens)
#   filter1 = [re.sub('[0-9]','', i) for i in tokens]
#   filter2=[lemmatizer.lemmatize(w.lower()) for w in filter1 if(not w in dic and w.isalpha())]
  


#   return filter2

# preprocessing("THIS IS playing plays123")

# new_data=t1.copy(deep=True)

# new_data=new_data.iloc[700000:,:]

# new_data.shape

# new_data.shape

# new_data['text']=new_data['text'].apply(preprocessing)

# new_data.shape

# new_data.to_csv('/content/drive/My Drive/RK_ALDA_PROJ/release/till_last.csv')

# new_data.head()



"""**WORD CLOUDS**"""

data.shape

random100=data.sample(n=100)

random100.head()

random100.reset_index(drop=True,inplace=True)

random100.head()

from wordcloud import WordCloud,STOPWORDS

"""**Low Coverage Word Clouds**"""

w1=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_low.loc[0,'summary'])

import matplotlib.pyplot as plt

plt.imshow(w1)

w2=WordCloud(width=1000,height=1000,background_color='white',stopwords=set(STOPWORDS),min_font_size=30).generate(random_low.loc[0,'text'])

plt.imshow(w2)

random_low=random100[random100['coverage_bin']=="low"]

random_low.head()

random_medium=random100[random100['coverage_bin']=="medium"]

random_high=random100[random100['coverage_bin']=="high"]

random_high.head()

random_low.reset_index(drop=True,inplace=True)
random_medium.reset_index(drop=True,inplace=True)
random_high.reset_index(drop=True,inplace=True)

w1=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_low.loc[1,'summary'])

plt.imshow(w1)

w2=WordCloud(width=1000,height=1000,background_color='white',stopwords=set(STOPWORDS),min_font_size=30).generate(random_low.loc[1,'text'])

plt.imshow(w2)

"""**Medium Coverage Word Clouds**"""

random_medium

w1=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_medium.loc[0,'summary'])

plt.imshow(w1)

w2=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_medium.loc[0,'text'])

plt.imshow(w2)

w1=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_medium.loc[3,'summary'])

plt.imshow(w1)

w2=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_medium.loc[3,'text'])

plt.imshow(w2)

"""High Coverage"""

random_high.head()

w1=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_high.loc[5,'summary'])

plt.imshow(w1)

w2=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_high.loc[5,'text'])

plt.imshow(w2)

w1=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_high.loc[3,'summary'])

plt.imshow(w1)

w2=WordCloud(width=800,height=800,background_color='white',stopwords=set(STOPWORDS),min_font_size=10).generate(random_high.loc[3,'text'])

plt.imshow(w2)

random_low.describe()

random_medium.describe()

random_high.describe()

del data['Unnamed: 0.1']
data.corr()

new=data.iloc[0:100000,:]

new.corr()

plt.scatter(new['coverage'][:5000],new['density'][:5000])
plt.xlabel("Coverage")
plt.ylabel("Density")

# plt.scatter(new['density'][:5000],new['coverage'][:5000])
# # plt.xlabel("Coverage")
# # plt.ylabel("Density")

new[new['coverage_bin']=="low"].describe()

new[new['coverage_bin']=="medium"].describe()

new[new['coverage_bin']=="high"].describe()

random_medium['coverage_bin']

